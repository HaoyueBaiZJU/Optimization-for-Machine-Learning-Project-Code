# Optimization-for-Machine-Learning-Project-Code
Optimization for Machine Learning Course Project

# Homework 1
We consider ridge regression problem with randomly generated data. The goal is to implement gradient descent and experiment with different strong-convexity settings and different learning rates. Test with different weights of regularizer.

Implement functions: 
1) smoothness parameter
2) closed form solution of the ridge regression problem
3) compute the objective function value
4) compute the gradient of the objective function
5) gradient descent algorithm for t iterations

# Homework 2
We consider optimization with the smoothed hinge loss, and randomly generated data. The goal is to implement gradient descent and experiment with different strong-convexity settings and different learning rates.

Implement functions:
1) implement the smoothed hinge loss (SVM) objective function
2) compute the gradient of the smoothed hinge loss object function
3) heavy ball method with adaptive beta (practically simplified)
4) Nesterov's acceleration method
5) Nesterov's acceleration with adaptive beta (practically simplified)
6) Nesterov's general acceleration method (applicable for smooth and non-strongly convex case)

# Homework 3
Download data in the mnist directory (which contains class 1 (positive) versus 7 (negative) from the MNIST data)








